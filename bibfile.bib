

@online{bushaev-2018,
	author = {given=Vitaly, family=Bushaev},
	date = {2018-10-22},
	language = {english},
	title = {Adam — latest trends in deep learning optimization.},
	url = {https://towardsdatascience.com/adam-latest-trends-in-deep-learning-optimization-6be9a291375c},
	urldate = {2022-06-29},
}

@online{linkedin-no-date,
	author = {{LinkedIn}},
	language = {american},
	title = {Tariq Rashid},
	url = {https://www.linkedin.com/in/tariq-rashid-3301b9195/?originalSubdomain=uk},
	urldate = {2022-06-29},
}
@online{educative-no-date,
	author = {{Educative}},
	language = {english},
	title = {Tariq Rashid - Educative Author},
	url = {https://www.educative.io/profile/view/5693482056286208},
	urldate = {2022-06-29},
}

@report{fraunhofer-allianz-big-data-2017,
	author = {Dirk Hecker and
Inga Döbel and
Ulrike Petersen and
André Rauschert and
Velina Schmitz and
Angelika Voss},
	date = {2017-11},
	language = {american},
	publisher = {Fraunhofer-Allianz Big Data},
	title = {Zukunftsmarkt Künstliche Intelligenz: Potenziale und Anwendungen},
	url = {https://www.bigdata-ai.fraunhofer.de/content/dam/bigdata/de/documents/Publikationen/KI-Potenzialanalyse_2017.pdf},
}

@online{befores-afters-2021,
	author = {{Befores \& Afters}},
	date = {2021-06-09},
	language = {american},
	title = {A breakdown of the fun ways you can already use CopyCat in Nuke},
	url = {https://beforesandafters.com/2021/06/09/a-breakdown-of-the-fun-ways-you-can-already-use-copycat-in-nuke/},
	urldate = {2022-06-29},
}

@online{fevbre-no-date,
	author = {given=Quentin, family= Fevbre},
	language = {english},
	title = {Understanding memory usage in deep learning models training},
	url = {https://www.sicara.fr/blog-technique/2019-28-10-deep-learning-memory-usage-and-pytorch-optimization-tricks},
	urldate = {2022-06-28},
}

@online{doshi-2019,
	author = {given=Sanket, family=Doshi},
	date = {2019-01-13},
	language = {english},
	title = {Various Optimization Algorithms For Training Neural Network},
	url = {https://towardsdatascience.com/optimizers-for-training-neural-network-59450d71caf6},
	urldate = {2022-06-27},
}

@image{iprathore-2020,
	author = {family=Iprathore},
	date = {2020-10-13},
	language = {american},
	title = {Types of Residual Blocks},
	url = {https://miro.medium.com/max/1400/1*RoVWW8sv_lt3z_A73p3h1Q.png},
}

@online{nandepu-2021,
	author = {given=Raghunandepu, family=Nandepu},
	date = {2021-12-12},
	language = {english},
	title = {Understanding and implementation of Residual Networks(ResNets)},
	url = {https://medium.com/analytics-vidhya/understanding-and-implementation-of-residual-networks-resnets-b80f9a507b9c},
	urldate = {2022-06-27},
}

@online{side-effects-software-2017,
	author = {{Side Effects Software}},
	date = {2017-10-31},
	language = {english},
	title = {Moana | SideFX},
	url = {https://www.sidefx.com/community/walt-disney-animation-studios-moana/},
	urldate = {2022-06-27},
}

@misc{Moana,
  title =   {Moana},
  producer= {Osnat Shurer},     
  director = {John Musker, Ron Clements},
  year = {2016},
  publisher = {Walt Disney Pictures}
}

@online{feldman-2019,
	author = {given=Dana, family=Feldman},
	date = {2019-04-17},
	language = {english},
	title = {How To Make A 'Game Of Thrones' Dragon: A VFX Supervisor Explains},
	url = {https://www.forbes.com/sites/danafeldman/2019/04/15/how-to-make-a-game-of-thrones-dragon/?sh=467bd1b5a2bb},
	urldate = {2022-06-26},
}

@online{fis-eireann-2019,
	author = {{Fís Éireann}},
	date = {2019-02-04},
	language = {american},
	title = {Visual Effects (VFX)},
	url = {https://www.careersinscreen.ie/visual-effects/#sec39},
	urldate = {2022-06-26},
}

@online{chaubey-2021,
	author = {given=Aashish, family=Chaubey},
	date = {2020-01-14},
	language = {english},
	title = {Downsampling and Upsampling of Images — Demystifying the Theory},
	url = {https://medium.com/analytics-vidhya/downsampling-and-upsampling-of-images-demystifying-the-theory-4ca7e21db24a},
	urldate = {2022-06-25},
}

@online{alps-vfx-2020,
	author = {{ALPS VFX}},
	date = {2020-02-06},
	language = {british},
	title = {Explosion RnD},
	url = {https://www.alps-vfx.com/work/misc/explosion-rnd/},
	urldate = {2022-06-25},
}

@online{dawson-2021,
	author = {given=Ross, family=Dawson},
	date = {2021-03-25},
	language = {american},
	title = {A comprehensive guide to the state-of-art in how AI is transforming the visual effects (VFX) industry},
	url = {https://rossdawson.com/futurist/implications-of-ai/comprehensive-guide-ai-artificial-intelligence-visual-effects-vfx/},
	urldate = {2022-06-25},
}

@online{openai-2022,
	author = {{OpenAI}},
	date = {2022-02-14},
	language = {english},
	title = {Summarizing Books with Human Feedback},
	url = {https://openai.com/blog/summarizing-books/},
	urldate = {2022-06-25},
}

@online{ralf-2022,
	author = {given=Ralf, family=Ralf},
	date = {2022-05-10},
	language = {american},
	title = {Your iPhone Uses Artificial Intelligence in Ways You Don’t Even Know},
	url = {https://www.techiexpert.com/your-iphone-uses-artificial-intelligence-in-ways-you-dont-even-know/},
	urldate = {2022-06-25},
}

@online{sidefxsparse,
	author = {{Side Effects Software}},
	language = {english},
	title = {Pyro Solver (Sparse)},
	url = {https://www.sidefx.com/docs/houdini/nodes/dop/pyrosolver_sparse.html},
	urldate = {2022-06-23},
}

@online{sidefxparicle,
	author = {{Side Effects Software}},
	language = {english},
	title = {Pyro Solver (Sparse)},
	url = {https://www.sidefx.com/docs/houdini/dopparticles/instancing.html},
	urldate = {2022-06-23},
}




@online{kim-2022,
	author = {Rachel Kim},
	date = {2022-05-26},
	language = {american},
	title = {WHY IS HOUDINI THE FUTURE OF 3D AND VFX?},
	url = {https://infocusfilmschool.com/houdini-future-3d-vfx/},
	urldate = {2022-08-11},
}

@online{wood-2020,
	author = {given=Thomas, family=Wood},
	date = {2020-09-27},
	language = {american},
	title = {Sigmoid Function},
	url = {https://deepai.org/machine-learning-glossary-and-terms/sigmoid-function},
	urldate = {2022-06-21},
}

@online{meta-aiflatten,
	author = {{Meta AI.}},
	language = {english},
	title = {torch.flatten — PyTorch 1.11.0 documentation},
	url = {https://pytorch.org/docs/stable/generated/torch.flatten.html},
	urldate = {2022-06-21},
}

@online{liu-2019,
	author = {given=Danqing, family=Liu},
	date = {2017-11-30},
	language = {english},
	title = {A Practical Guide to ReLU - Danqing Liu},
	url = {https://medium.com/@danqing/a-practical-guide-to-relu-b83ca804f1f7},
	urldate = {2022-06-21},
}

@online{thomas-2021,
	author = {given=Christopher, family=Thomas},
	date = {2021-12-10},
	language = {english},
	title = {An introduction to Convolutional Neural Networks - Towards Data Science},
	url = {https://towardsdatascience.com/an-introduction-to-convolutional-neural-networks-eb0b60b58fd7},
	urldate = {2022-06-21},
}

@online{brownlee-2019,
	author = {given=Jason, family=Brownlee},
	date = {2019-10-25},
	language = {american},
	title = {Difference Between a Batch and an Epoch in a Neural Network},
	url = {https://machinelearningmastery.com/difference-between-a-batch-and-an-epoch/},
	urldate = {2022-06-21},
}

@online{nearestneighbor,
	author = {{Meta AI}},
	language = {english},
	title = {Upsample — PyTorch 1.11.0 documentation},
	url = {https://pytorch.org/docs/stable/generated/torch.nn.Upsample.html},
	urldate = {2022-06-21},
}

@misc{batchnorm,
  doi = {10.48550/ARXIV.1502.03167},
  
  url = {https://arxiv.org/abs/1502.03167},
  
  author = {Ioffe, Sergey and Szegedy, Christian},
  
  keywords = {Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift},
  
  publisher = {arXiv},
  
  year = {2015},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@online{thuerey-group-no-date,
	author = {{Thuerey Group}},
	language = {american},
	title = {Mantaflow Tensorflow Tutorial 2 – Thuerey Group},
	url = {https://ge.in.tum.de/research/mantaflow-tensorflow-tutorial-2/},
	urldate = {2022-06-20},

}

@Article{         harris2020array,
 title         = {Array programming with {NumPy}},
 author        = {Charles R. Harris and K. Jarrod Millman and St{\'{e}}fan J.
                 van der Walt and Ralf Gommers and Pauli Virtanen and David
                 Cournapeau and Eric Wieser and Julian Taylor and Sebastian
                 Berg and Nathaniel J. Smith and Robert Kern and Matti Picus
                 and Stephan Hoyer and Marten H. van Kerkwijk and Matthew
                 Brett and Allan Haldane and Jaime Fern{\'{a}}ndez del
                 R{\'{i}}o and Mark Wiebe and Pearu Peterson and Pierre
                 G{\'{e}}rard-Marchant and Kevin Sheppard and Tyler Reddy and
                 Warren Weckesser and Hameer Abbasi and Christoph Gohlke and
                 Travis E. Oliphant},
 year          = {2020},
 month         = {09},
 journal       = {Nature},
 volume        = {585},
 number        = {7825},
 pages         = {357--362},
 doi           = {10.1038/s41586-020-2649-2},
 publisher     = {Springer Science and Business Media {LLC}},
 url           = {https://doi.org/10.1038/s41586-020-2649-2}
}

@misc{mantaflow,
     Author = {Nils Thuerey and Tobias Pfaff},
     Title = {{MantaFlow}},
     Year = {2018},
     url = {http://mantaflow.com}
}

@software{reback2020pandas,
    author       = {The pandas development team},
    title        = {pandas-dev/pandas: Pandas},
    month        = feb,
    year         = 2020,
    publisher    = {Zenodo},
    version      = {1.4.1},
    doi          = {10.5281/zenodo.3509134},
    url          = {https://doi.org/10.5281/zenodo.3509134}
}


@software{TensorBoard,
  author = {{Google Brain Team}},
  title = {TensorBoard},
  url = {https://www.tensorflow.org/},
  version = {2.9.0},
  date = {2022-04-29},
}

@software{torch,
  author = {{Meta AI}},
  title = {PyTorch},
  url = {https://pytorch.org/},
  version = {1.11.0},
  date = {2022-03-10},
}

@software{houdini,
  author = {{Side Effects Software}},
  title = {Houdini},
  url = {https://www.sidefx.com/},
  version = {18.5},
  date = {2020-10-17},
}

@software{openvdb,
  author = {{Academy Software Foundation}},
  title = {OpenVDB},
  url = {www.openvdb.org},
  version = {9.0.0},
  date = {2022-10-30},
}

@software{python,
  author = {{Python Software Foundation}},
  title = {Python},
  url = {https://www.python.org/},
  version = {3.7.7},
  date = {2020-03-10},
}

@article{xie2018tempoGAN, 
title="{tempoGAN: A Temporally Coherent, Volumetric GAN for Super-resolution Fluid Flow}", 
author={Xie, You and Franz, Erik and Chu, Mengyu and Thuerey, Nils}, 
journal={ACM Transactions on Graphics (TOG)}, 
volume={37}, number={4}, pages={95}, year={2018}, publisher={ACM} }

@article{Gao_2021,
	doi = {10.1063/5.0054312},
  
	url = {https://doi.org/10.1063%2F5.0054312},
  
	year = 2021,
	month = {7},
  
	publisher = {{AIP} Publishing},
  
	volume = {33},
  
	number = {7},
  
	pages = {073603},
  
	author = {Han Gao and Luning Sun and Jian-Xun Wang},
  
	title = {Super-resolution and denoising of fluid flow using physics-informed convolutional neural networks without high-resolution labels},
  
	journal = {Physics of Fluids}
}



@article{baidirectory,
author = {Bai, Kai and Li, Wei and Desbrun, Mathieu and Liu, Xiaopei},
title = {Dynamic Upsampling of Smoke through Dictionary-Based Learning},
year = {2020},
issue_date = {February 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {40},
number = {1},
issn = {0730-0301},
url = {https://doi.org/10.1145/3412360},
doi = {10.1145/3412360},
abstract = {Simulating turbulent smoke flows with fine details is computationally intensive. For iterative editing or simply faster generation, efficiently upsampling a low-resolution numerical simulation is an attractive alternative. We propose a novel learning approach to the dynamic upsampling of smoke flows based on a training set of flows at coarse and fine resolutions. Our multiscale neural network turns an input coarse animation into a sparse linear combination of small velocity patches present in a precomputed over-complete dictionary. These sparse coefficients are then used to generate a high-resolution smoke animation sequence by blending the fine counterparts of the coarse patches. Our network is initially trained from a sequence of example simulations to both construct the dictionary of corresponding coarse and fine patches and allow for the fast evaluation of a sparse patch encoding of any coarse input. The resulting network provides an accurate upsampling when the coarse input simulation is well approximated by patches present in the training set (e.g., for re-simulation), or simply visually plausible upsampling when input and training sets differ significantly. We show a variety of examples to ascertain the strengths and limitations of our approach and offer comparisons to existing approaches to demonstrate its quality and effectiveness.},
journal = {ACM Trans. Graph.},
month = {9},
articleno = {4},
numpages = {19},
keywords = {Fluid simulation, smoke animation, neural networks, dictionary learning}
}



@online{baheti-2022,
	author = {given=Pragati, family=Baheti},
	date = {2022-05-26},
	language = {american},
	title = {What is Overfitting in Deep Learning and How to Avoid It},
	url = {https://www.v7labs.com/blog/overfitting},
	urldate = {2022-06-15},
}

@article{reeves,
author = {Reeves, W. T.},
title = {Particle Systems—a Technique for Modeling a Class of Fuzzy Objects},
year = {1983},
issue_date = {April 1983},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {2},
issn = {0730-0301},
url = {https://doi.org/10.1145/357318.357320},
doi = {10.1145/357318.357320},
journal = {ACM Trans. Graph.},
month = {4},
pages = {91–108},
numpages = {18}
}



@inproceedings{baraff-2001,
	author = {given=David, family=Baraff},
	date = {2001},
	eventtitle = {SIGGRAPH},
	language = {american},
	title = {Physically Based Modeling Rigid Body Simulation},
	url = {https://graphics.pixar.com/pbm2001/pdf/notesg.pdf},
}

@online{menace-2021,
	author = {{Top Menace}},
	date = {2021-12-09},
	language = {english},
	title = {Vertices, Edges and Polygons ( Basics of 3D ) Topmenace},
	url = {https://medium.com/@topmenace/vertices-edges-and-polygons-basic-of-3d-topmenace-c16e30d081ec},
	urldate = {2022-06-15},
}

@online{garcia-no-date,
	author = {given=Carolina Jiménez, family=García},
	language = {english},
	title = {What are Full CG Shots? | "Layout for VFX, Camera Narrative and Language" (okinfografia)},
	url = {https://www.domestika.org/en/courses/299-layout-for-vfx-camera-narrative-and-language/units/1513-layout/lessons/5805-what-are-full-cg-shots},
	urldate = {2022-06-15},
}

@online{jacobs-2022,
	author = { given=Matt, family=Jacobs},
	date = {2022-06-10},
	language = {american},
	title = {What Is CGI? Complete Guide To Computer Generated Imagery •},
	url = {https://filmlifestyle.com/what-is-cgi/},
	urldate = {2022-06-15},
}

@online{aditya-2022,
	author = {Aditya Ramesh and Mikhail Pavlov and Gabriel Goh and Scott Gray},
	date = {2022-06-09},
	language = {english},
	title = {DALL·E: Creating Images from Text},
	url = {https://openai.com/blog/dall-e/},
	urldate = {2022-06-15},
}

@online{rockwell-2020,
	author = {given=Rockwell, family= Anyoha},
	date = {2020-04-23},
	language = {american},
	title = {The History of Artificial Intelligence},
	url = {https://sitn.hms.harvard.edu/flash/2017/history-artificial-intelligence/},
	urldate = {2022-06-15},
}

@online{prov-international-inc-2022,
	author = {{ProV International Inc.}},
	date = {2022-04-19},
	language = {american},
	title = {5 Common Machine Learning Problems and How to Solve Them},
	url = {https://www.provintl.com/blog/5-common-machine-learning-problems-how-to-beat-them},
	urldate = {2022-06-14},
}

@online{hao-2020,
	author = {given=Karen, family=Hao},
	date = {2020-04-02},
	language = {english},
	title = {This is how AI bias really happens—and why it’s so hard to fix},
	url = {https://www.technologyreview.com/2019/02/04/137602/this-is-how-ai-bias-really-happensand-why-its-so-hard-to-fix/},
	urldate = {2022-06-14},
}

@online{ayari-2021,
	author = {given=Rabeh, family=Ayari, suffix=PhD},
	date = {2021-12-13},
	language = {english},
	title = {Generative Adversarial Networks - Towards Data Science},
	url = {https://towardsdatascience.com/generative-adversarial-networks-gans-2231c5943b11},
	urldate = {2022-06-14},
}

@online{dickson-2019,
	author = {given=Ben, family=Dickson},
	date = {2019-08-11},
	language = {american},
	title = {The limits and challenges of deep learning},
	url = {https://bdtechtalks.com/2018/02/27/limits-challenges-deep-learning-gary-marcus/},
	urldate = {2022-06-14},
}

@online{giacaglia-2021,
	author = {given=Giuliano, family=Giacaglia},
	date = {2021-12-13},
	language = {english},
	title = {How to scale training on multiple GPUs - Towards Data Science},
	url = {https://towardsdatascience.com/how-to-scale-training-on-multiple-gpus-dae1041f49d2},
	urldate = {2022-06-14},
}
@online{ibm-cloud-education-2021,
	author = {{IBM Cloud Education}},
	date = {2021-01-06},
	language = {american},
	title = {Convolutional Neural Networks},
	url = {https://www.ibm.com/cloud/learn/convolutional-neural-networks},
	urldate = {2022-06-14},
}

@online{ramsundar-no-date,
	author = {given=Bharath, family=Ramsundar and given=Reza Bosagh, family=Zadeh},
	language = {english},
	title = {TensorFlow for Deep Learning},
	url = {https://www.oreilly.com/library/view/tensorflow-for-deep/9781491980446/ch04.html},
	urldate = {2022-06-14}
}

@online{silva-2020,
	author = {given= Thalles, family=Silva},
	date = {2020-04-08},
	language = {english},
	title = {An intuitive introduction to Generative Adversarial Networks (GANs)},
	url = {https://www.freecodecamp.org/news/an-intuitive-introduction-to-generative-adversarial-networks-gans-7a2264a81394},
	urldate = {2022-06-14}
}

@online{luber-2021,
	author = {given=Stefan, family=Luber},
	date = {2021-02-12},
	language = {german},
	title = {Was ist ein Generative Adversarial Network (GAN)?},
	url = {https://www.bigdata-insider.de/was-ist-ein-generative-adversarial-network-gan-a-999817/},
	urldate = {2022-06-13}
}

@inproceedings{Goodfellowgan,
 author = {Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Generative Adversarial Nets},
 url = {https://proceedings.neurips.cc/paper/2014/file/5ca3e9b122f61f8f06494c97b1afccf3-Paper.pdf},
 volume = {27},
 year = {2014}
}

@Book{GoodBengCour16,
  Title                    = {Deep Learning},
  Author                   = {Ian Goodfellow and Yoshua Bengio and Aaron Courville},
  Publisher                = {MIT Press},
  Year                     = {2016},

  Address                  = {Cambridge, MA, USA},
  url                     = {http://www.deeplearningbook.org}
}

@article{kukreja2016introduction,
  title={An introduction to artificial neural network},
  author={Kukreja, Harsh and Bharath, N and Siddesh, CS and Kuldeep, S},
  journal={Int J Adv Res Innov Ideas Educ},
  volume={1},
  pages={27--30},
  year={2016}
}

@article{Hahnloser2000,
  doi = {10.1038/35016072},
  url = {https://doi.org/10.1038/35016072},
  year = {2000},
  month = jun,
  publisher = {Springer Science and Business Media {LLC}},
  volume = {405},
  number = {6789},
  pages = {947--951},
  author = {Richard H. R. Hahnloser and Rahul Sarpeshkar and Misha A. Mahowald and Rodney J. Douglas and H. Sebastian Seung},
  title = {Digital selection and analogue amplification coexist in a cortex-inspired silicon circuit},
  journal = {Nature}
}

@book {tariq_gan, 
	title = "GANs mit PyTorch selbst programmieren",
	author = "Rashid, Tariq",
	isbn = "9783960103936",
	year = "2020",
	edition = "1",
	keywords = "Big Data, Python, Künstliche Intelligenz, Neuronale Netze, Machine Learning, Data Mining, Artificial Intelligence, Deep Learning, Maschinelles Lernen, AI, Künstliche Neuronale Netze, PyTorch, GAN, Generative Adversarial Networks",
	url = "http://www.content-select.com/index.php?id=bib_view&amp;ean=9783960103936",
	url = "https://content-select.com/de/portal/media/view/5f4a4d24-1e80-43ba-b0a8-0ee6b0dd2d03",
	publisher = "O'Reilly Verlag",
	language = "ger",
}





@book {traiq_neuron, 
	title = "Neuronale Netze selbst programmieren",
	author = "Rashid, Tariq",
	isbn = "9783960101024",
	year = "2017",
	keywords = "Big Data, Python, Künstliche Intelligenz, Neuronale Netze, Machine Learning, Data Mining, Artificial Intelligence, Deep Learning, Maschinelles Lernen, AI, Künstliche Neuronale Netze, Raspbery Pi",
	url = "http://www.content-select.com/index.php?id=bib_view&amp;ean=9783960101024",
	url = "https://content-select.com/de/portal/media/view/5d5fc35a-3c80-447a-85ea-246bb0dd2d03",
	publisher = "O'Reilly Verlag",
	language = "ger",
}



 
 @misc{crew2020, url={https://www.natureindex.com/news-blog/google-scholar-reveals-most-influential-papers-research-citations-twenty-twenty}, title={Google scholar reveals its most influential papers for 2020}, publisher={Nature Index}, author={Crew, Bec}, year={2020}, month={7},  urldate = {2022-06-06}} 

@article{Janiesch.2021,
 author = {Janiesch, Christian and Zschech, Patrick and Heinrich, Kai},
 year = {2021},
 title = {Machine learning and deep learning},
 pages = {685--695},
 pagination = {page},
 volume = {31},
 issn = {1019-6781},
 journaltitle = {Electronic Markets},
 shortjournal = {Electron Markets},
 doi = {10.1007/s12525-021-00475-2},
 number = {3},
 abstract = {},

}

 @misc{copeland_2021, title={The difference between AI, Machine Learning, and deep learning?}, url={https://blogs.nvidia.com/blog/2016/07/29/whats-difference-artificial-intelligence-machine-learning-deep-learning-ai/}, journal={NVIDIA Blog}, publisher={NVIDIA }, author={Copeland, Michael}, year={2021}, month={7}, urldate = {2022-06-06}}   
 
  @misc{wuttke_2022, title={Machine learning vs. Deep Learning: Wo ist der Unterschied?}, url={https://datasolut.com/machine-learning-vs-deep-learning/}, journal={Machine Learning vs. Deep Learning: Wo ist der Unterschied?}, publisher={datasolut GmbH}, author={Wuttke, Laurenz}, year={2022}, month={5}, urldate = {2022-06-05}} 
 
 @misc{wichert, url={https://www.spektrum.de/lexikon/neurowissenschaft/kuenstliche-intelligenz/6810}, journal={Künstliche Intelligenz}, publisher={Spektrum Akademischer Verlag}, author={Wichert, Andreas}, urldate = {2022-06-05}} 

@book{russell2012knstliche,
  added-at = {2018-07-18T12:58:34.000+0200},
  address = {München},
  author = {Russell, Stuart J. and Norvig, Peter},
  biburl = {https://www.bibsonomy.org/bibtex/238c18eef34cc510a9588a397e77b8a4f/chr},
  edition = {3., aktualisierte Auflage},
  editor = {Langenau, Frank},
  interhash = {088be36e25f0f8f3238529999aa45029},
  intrahash = {38c18eef34cc510a9588a397e77b8a4f},
  isbn = {978-3-86894-098-5},
  keywords = {ai ba ki},
  price = {EUR 59,95},
  publisher = {Pearson, Higher Education},
  timestamp = {2018-07-18T13:06:15.000+0200},
  title = {Künstliche Intelligenz: Ein moderner Ansatz},
  year = 2012
}

@book{psychologie,
  author={Myers, David G. and Hoppe-Graff, Siegfried},
  title={Psychologie},
  edition={3., vollst. {\"u}berarb. und erw. Aufl.},
  publisher={Springer},
  address={Berlin ; Heidelberg},
  year={2014},
  pages={XXXVI, 1034 S.},
  language={ger},
  series={Springer-Lehrbuch},
  note={Literaturverz. S. 909 - 1007},
  doi={10.1007/978-3-642-40782-6},
  keywords={Psychologie},
  library={UB [Signatur: LA-C 21-21461::(3)] ; PS [Signatur: A I 1033 Mye] ; PS [Signatur: A I 1052 Mye]},
}


@article{deepfluid, 
author = {Kim, Byungsoo and Azevedo, Vinicius C. and Thuerey, Nils and Kim, Theodore and Gross, Markus and Solenthaler, Barbara},
title = {Deep Fluids: A Generative Network for Parameterized Fluid Simulations},
journal = {Computer Graphics Forum},
volume = {38},
number = {2},
pages = {59-70},
keywords = {CCS Concepts, • Computing methodologies → Physical simulation, Neural networks},
doi = {https://doi.org/10.1111/cgf.13619}, 
url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/cgf.13619},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1111/cgf.13619},
abstract = {Abstract This paper presents a novel generative model to synthesize fluid simulations from a set of reduced parameters. A convolutional neural network is trained on a collection of discrete, parameterizable fluid simulation velocity fields. Due to the capability of deep learning architectures to learn representative features of the data, our generative model is able to accurately approximate the training data set, while providing plausible interpolated in-betweens. The proposed generative model is optimized for fluids by a novel loss function that guarantees divergence-free velocity fields at all times. In addition, we demonstrate that we can handle complex parameterizations in reduced spaces, and advance simulations in time by integrating in the latent space with a second network. Our method models a wide variety of fluid behaviors, thus enabling applications such as fast construction of simulations, interpolation of fluids with different parameters, time re-sampling, latent space simulations, and compression of fluid simulation data. Reconstructed velocity fields are generated up to 700× faster than re-simulating the data with the underlying CPU solver, while achieving compression rates of up to 1300×.},
year = {2019}
}


@article{physicsinformed,
  author    = {Maziar Raissi and
               Paris Perdikaris and
               George E. Karniadakis},
  title     = {Physics Informed Deep Learning (Part {I):} Data-driven Solutions of
               Nonlinear Partial Differential Equations},
  journal   = {CoRR},
  volume    = {abs/1711.10561},
  year      = {2017},
  url       = {http://arxiv.org/abs/1711.10561},
  eprinttype = {arXiv},
  eprint    = {1711.10561},
  timestamp = {Mon, 13 Aug 2018 16:47:03 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1711-10561.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{Real-Time-incompressible,
author = {Nie, Xiao and Chen, Leiting and Xiang, Tao},
title = {Real-Time Incompressible Fluid Simulation on the GPU},
year = {2015},
issue_date = {January 2015},
publisher = {Hindawi Limited},
address = {London, GBR},
volume = {2015},
issn = {1687-7047},
url = {https://doi.org/10.1155/2015/417417},
doi = {10.1155/2015/417417},
abstract = {We present a parallel framework for simulating incompressible fluids with predictive-corrective incompressible smoothed particle hydrodynamics (PCISPH) on the GPU in real time. To this end, we propose an efficient GPU streaming pipeline to map the entire computational task onto the GPU, fully exploiting the massive computational power of state-of-the-art GPUs. In PCISPH-based simulations, neighbor search is the major performance obstacle because this process is performed several times at each time step. To eliminate this bottleneck, an efficient parallel sorting method for this time-consuming step is introduced. Moreover, we discuss several optimization techniques including using fast on-chip shared memory to avoid global memory bandwidth limitations and thus further improve performance on modern GPU hardware. With our framework, the realism of real-time fluid simulation is significantly improved since our method enforces incompressibility constraint which is typically ignored due to efficiency reason in previous GPU-based SPH methods. The performance results illustrate that our approach can efficiently simulate realistic incompressible fluid in real time and results in a speed-up factor of up to 23 on a high-end NVIDIA GPU in comparison to single-threaded CPU-based implementation.},
journal = {Int. J. Comput. Games Technol.},
month = {1},
articleno = {2},
numpages = {1}
}

@article{10.1145/2487228.2487235,
author = {Museth, Ken},
title = {VDB: High-Resolution Sparse Volumes with Dynamic Topology},
year = {2013},
issue_date = {June 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {32},
number = {3},
issn = {0730-0301},
url = {https://doi.org/10.1145/2487228.2487235},
doi = {10.1145/2487228.2487235},
abstract = {We have developed a novel hierarchical data structure for the efficient representation of sparse, time-varying volumetric data discretized on a 3D grid. Our “VDB”, so named because it is a Volumetric, Dynamic grid that shares several characteristics with B+trees, exploits spatial coherency of time-varying data to separately and compactly encode data values and grid topology. VDB models a virtually infinite 3D index space that allows for cache-coherent and fast data access into sparse volumes of high resolution. It imposes no topology restrictions on the sparsity of the volumetric data, and it supports fast (average O(1)) random access patterns when the data are inserted, retrieved, or deleted. This is in contrast to most existing sparse volumetric data structures, which assume either static or manifold topology and require specific data access patterns to compensate for slow random access. Since the VDB data structure is fundamentally hierarchical, it also facilitates adaptive grid sampling, and the inherent acceleration structure leads to fast algorithms that are well-suited for simulations. As such, VDB has proven useful for several applications that call for large, sparse, animated volumes, for example, level set dynamics and cloud modeling. In this article, we showcase some of these algorithms and compare VDB with existing, state-of-the-art data structures.},
journal = {ACM Trans. Graph.},
month = {7},
articleno = {27},
numpages = {22},
keywords = {implicit surfaces, fluid animation, level sets, Volumes}
}

@online{AlexandreJoelChorin.1967,
 author = {{Alexandre Joel Chorin}},
 year = {1967},
 title = {The numerical solution of the Navier-Stokes equations for an incompressible fluid},
 url = {https://math.berkeley.edu/~chorin/chorin67.pdf},
 urldate = {2022-05-30},
 abstract = {Image},
 pagetotal = {8},
 location = {New York},
 organization = {{Courant Institute of Mathematical Sciences}},
 file = {chorin67:Attachments/chorin67.pdf:application/pdf}
}

@misc{flipsolver,
 publisher = {{Side Effects Software}},
 title = {FLIP Solver},
 url = {https://www.sidefx.com/docs/houdini/nodes/dop/flipsolver.html},
 urldate = {2022-05-30}
}



@misc{volumesopenvdb, title={Volumes},url={https://www.sidefx.com/docs/houdini/model/volumes.html}, journal={Houdini Help}, publisher={{Side Effects Software}}, urldate={2022-05-31}}


@misc{understandinghowpyroworks, title={Understanding how pyro works},url={https://www.sidefx.com/docs/houdini/pyro/background.html}, journal={Houdini Help}, publisher={{Side Effects Software}}, urldate={2022-05-28}}

@book{FoleyDamEtAl90,abstract = {The second edition of Fundamentals of Interactive Computer Graphics is completely rewritten to provide the most comprehensive, authoritative, and up-to-date coverage of the field. The authors provide a unique combination of current concepts and practical applications. The important algorithms in 2-D and 3-D graphics are detailed for easy implementation, including a close look at the more subtle special cases. There is also a thorough presentation of the mathematical principles of geometric transformations and viewing. In this book, the authors explore multiple perspectives on the field of computer graphics: the user's, the application programmer's, the package implementor's, and the hardware designer's. There are over 100 full-color plates and over 700 figures illustrating the techniques presented in the book. Its many outstanding features ensure its position as the standard computer graphics reference for practitioners, and as a comprehensive and understandable text for students of all levels.},
  added-at = {2011-01-30T21:53:45.000+0100},address = {Reading, MA},
  author = {Foley, James D. and van Dam, Andries and Feiner, Steven and Hughes, John},
  biburl = {https://www.bibsonomy.org/bibtex/210fc1671c7040f90a1454754a79b7262/bsc},
  edition = {2.},
  file = {InformIT Product page:http\://www.informit.com/title/0201121107:URL;Amazon Search inside:http\://www.amazon.de/gp/reader/0201121107/:URL;Google Books:http\://books.google.de/books?isbn=978-0-201-12110-0:URL},
  interhash = {6688db571874a5fe0e40404136b5e60f},
  intrahash = {10fc1671c7040f90a1454754a79b7262},
  isbn = {978-0-201-12110-0},
  keywords = {computer graphics practice},
  publisher = {Addison-Wesley},
  timestamp = {2011-10-06T16:20:54.000+0200},
  title = {Computer Graphics: Principles and Practice},
  year = 1990
}


@book{Discrete-Event,
title = "Discrete-Event System Simulation",
author = "J. Banks and Carson, {J. S.} and Nelson, {B. L.} and D. Nicol",
year = "2010",
language = "English",
isbn = "0136062121",
publisher = "Prentice Hall",
edition = "5",
}

@book{mitchell2013visual,
  title={Visual effects for film and television},
  author={Mitchell, Mitch},
  year={2013},
  publisher={Routledge}
}

@online{lee-2018,
	author = {Lee, Chris},
	date = {2018-12-12},
	language = {american},
	title = {Digital Doubles Are Revolutionizing Hollywood. But Why Do Movie Stars Hate Them?},
	url = {https://www.vulture.com/2018/12/why-do-movie-stars-hate-being-digitally-scanned.html},
	urldate={2022-03-24}
}

@online{rawlinson_2020, title={Special effects vs visual effects: REALTIME: VFX Studio}, url={https://www.realtimeuk.com/blog/special-effects-vs-visual-effects/}, journal={REALTIME}, publisher={realtimeuk}, author={Rawlinson, Jonathan}, year={2020}, month={04},
urldate={2022-03-24}} 
 
@online{mccormack_2013, 
title={Did 'vertigo' introduce computer graphics to cinema?}, url={https://rhizome.org/editorial/2013/may/9/did-vertigo-introduce-computer-graphics-cinema/}, journal={Rhizome}, publisher={https://rhizome.org/}, 
author={McCormack, Tom}, 
year={2013}, 
month={5},
urldate={2022-03-18}
} 

@online{wiki:The_Man_with_the_Rubber_Head,
   author = "Wikipedia",
   title = "{The Man with the Rubber Head} --- {W}ikipedia{,} The Free Encyclopedia",
   year = "2022",
   howpublished = {\url{http://en.wikipedia.org/w/index.php?title=The\%20Man\%20with\%20the\%20Rubber\%20Head&oldid=1070196180}},
urldate={2022-03-17}
 }

@online{capturing, 
title={Capturing the Moment},
url={https://americanhistory.si.edu/muybridge/htm/htm_sec1/sec1p2.htm}, 
journal={Freeze frame- capturing the moment}, 
publisher={National Museum of American History},
urldate={2022-03-17}
} 

@book{okun2020ves,
  title={The VES Handbook of Visual Effects: Industry Standard VFX Practices and Procedures},
  author={Okun, J.A. and Zwerman, S. and McKittrick, C. and Sepp-Wilson, L.},
  isbn={9781138542204},
  lccn={2019055721},
  url={https://books.google.de/books?id=A5lvzAEACAAJ},
  year={2020},
  publisher={Taylor \& Francis Group}
}

@book{stampfer1833,
  title={Die stroboscopischen Scheiben; oder, Optischen Zauberscheiben: Deren Theorie und wissenschaftliche Anwendung},
  author={Stampfer, S.},
  url={https://books.google.de/books?id=xUk0AQAAMAAJ},
  year={1833},
  publisher={Trentsensky \& Vieweg}
}

@article{Chu.2017,
 abstract = {We present a novel data-driven algorithm to synthesize high-resolution flow simulations with reusable repositories of space-time flow data. In our work, we employ a descriptor learning approach to encode the similarity between fluid regions with differences in resolution and numerical viscosity. We use convolutional neural networks to generate the descriptors from fluid data such as smoke density and flow velocity. At the same time, we present a deformation limiting patch advection method which allows us to robustly track deformable fluid regions. With the help of this patch advection, we generate stable space-time data sets from detailed fluids for our repositories. We can then use our learned descriptors to quickly localize a suitable data set when running a new simulation. This makes our approach very efficient, and resolution independent. We will demonstrate with several examples that our method yields volumes with very high effective resolutions, and non-dissipative small scale details that naturally integrate into the motions of the underlying flow.},
 author = {Chu, Mengyu and Thuerey, Nils},
 year = {2017},
 title = {Data-Driven Synthesis of Smoke Flows with CNN-based Feature Descriptors},
 url = {https://arxiv.org/pdf/1705.01425},
 pages = {1--14},
 volume = {36},
 number = {4},
 issn = {0730-0301},
 journal = {ACM Trans. Graph.},
 doi = {10.1145/3072959.3073643},
 file = {Chu, Thuerey 2017 - Data-Driven Synthesis of Smoke Flows:Attachments/Chu, Thuerey 2017 - Data-Driven Synthesis of Smoke Flows.pdf:application/pdf}
}


@misc{Chu.23.11.2018,
 abstract = {Our work explores temporal self-supervision for GAN-based video generation tasks. While adversarial training successfully yields generative models for a variety of areas, temporal relationships in the generated data are much less explored. Natural temporal changes are crucial for sequential generation tasks, e.g. video super-resolution and unpaired video translation. For the former, state-of-the-art methods often favor simpler norm losses such as {\$}L{\^{}}2{\$} over adversarial training. However, their averaging nature easily leads to temporally smooth results with an undesirable lack of spatial detail. For unpaired video translation, existing approaches modify the generator networks to form spatio-temporal cycle consistencies. In contrast, we focus on improving learning objectives and propose a temporally self-supervised algorithm. For both tasks, we show that temporal adversarial learning is key to achieving temporally coherent solutions without sacrificing spatial detail. We also propose a novel Ping-Pong loss to improve the long-term temporal consistency. It effectively prevents recurrent networks from accumulating artifacts temporally without depressing detailed features. Additionally, we propose a first set of metrics to quantitatively evaluate the accuracy as well as the perceptual quality of the temporal evolution. A series of user studies confirm the rankings computed with these metrics. Code, data, models, and results are provided at https://github.com/thunil/TecoGAN. The project page https://ge.in.tum.de/publications/2019-tecogan-chu/ contains supplemental materials.},
 author = {Chu, Mengyu and Xie, You and Mayer, Jonas and Leal-Taix{\'e}, Laura and Thuerey, Nils},
 date = {2020},
 title = {Learning Temporal Coherence via Self-Supervision for GAN-based Video  Generation},
 url = {https://arxiv.org/pdf/1811.09393},
 number = {4},
 doi = {10.1145/3386569.3392457},
 file = {Chu, Xie et al. 23.11.2018 - Learning Temporal Coherence via Self-Supervision:Attachments/Chu, Xie et al. 23.11.2018 - Learning Temporal Coherence via Self-Supervision.pdf:application/pdf}
}


@misc{Ho.19.06.2020,
 abstract = {We present high quality image synthesis results using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. Our best results are obtained by training on a weighted variational bound designed according to a novel connection between diffusion probabilistic models and denoising score matching with Langevin dynamics, and our models naturally admit a progressive lossy decompression scheme that can be interpreted as a generalization of autoregressive decoding. On the unconditional CIFAR10 dataset, we obtain an Inception score of 9.46 and a state-of-the-art FID score of 3.17. On 256x256 LSUN, we obtain sample quality similar to ProgressiveGAN. Our implementation is available at https://github.com/hojonathanho/diffusion},
 author = {Ho, Jonathan and Jain, Ajay and Abbeel, Pieter},
 date = {19.06.2020},
 title = {Denoising Diffusion Probabilistic Models},
 url = {https://arxiv.org/pdf/2006.11239},
 file = {Ho, Jain et al. 19.06.2020 - Denoising Diffusion Probabilistic Models:Attachments/Ho, Jain et al. 19.06.2020 - Denoising Diffusion Probabilistic Models.pdf:application/pdf}
}


@article{Kim.2008,
 abstract = {We present a novel wavelet method for the simulation of fluids at high spatial resolution. The algorithm enables large- and small-scale detail to be edited separately, allowing high-resolution detail to be added as a post-processing step. Instead of solving the Navier-Stokes equations over a highly refined mesh, we use the wavelet decomposition of a low-resolution simulation to determine the location and energy characteristics of missing high-frequency components. We then synthesize these missing components using a novel incompressible turbulence function, and provide a method to maintain the temporal coherence of the resulting structures. There is no linear system to solve, so the method parallelizes trivially and requires only a few auxiliary arrays. The method guarantees that the new frequencies will not interfere with existing frequencies, allowing animators to set up a low resolution simulation quickly and later add details without changing the overall fluid motion.},
 author = {Kim, Theodore and Th{\"u}rey, Nils and James, Doug and Gross, Markus},
 year = {2008},
 title = {Wavelet Turbulence for Fluid Simulation},
 keywords = {fluids;noise;simulation control;turbulence;wavelets},
 pages = {1--6},
 volume = {27},
 number = {3},
 issn = {0730-0301},
 journal = {ACM Trans. Graph.},
 doi = {10.1145/1360612.1360649}
}


@book{Rashid.2017,
 author = {Rashid, Tariq},
 year = {2017},
 title = {Neuronale Netze selbst programmieren: Ein verst{\"a}ndlicher Einstieg mit Python},
 url = {https://ebookcentral.proquest.com/lib/kxp/detail.action?docID=5102090},
 address = {Heidelberg},
 edition = {1. Auflage},
 publisher = {o'Reilly},
 isbn = {9783960101024}
}


@book{Rashid.2020,
 abstract = {Intro -- Inhalt -- Einf{\"u}hrung -- Teil I: PyTorch und neuronale Netze -- Kapitel 1: Grundlagen von PyTorch -- Google Colab -- PyTorch-Tensoren -- Automatische Gradienten mit PyTorch -- Berechnungsgraphen -- Lernziele -- Kapitel 2: Erstes neuronales Netz mit PyTorch -- Das MNIST-Bilddatensatz -- Die MNIST-Daten abrufen -- Ein Blick auf die Daten -- Ein einfaches neuronales Netz -- Das Training visualisieren -- Die Klasse f{\"u}r den MNIST-Datensatz -- Unsere Klassifizierer trainieren -- Das neuronale Netz abfragen -- Die Performance des Klassifizierers einfach ermitteln -- Kapitel 3: Verfeinerungen -- Verlustfunktion -- Aktivierungsfunktion -- Optimierungsmethode -- Normalisierung -- Kombinierte Verfeinerungen -- Lernziele -- Kapitel 4: Grundlagen von CUDA -- NumPy vs. Python -- NVIDIA CUDA -- CUDA in Python verwenden -- Lernziele -- Teil II: Generative Adversarial Networks erstellen -- Kapitel 5: Das GAN-Konzept -- Bilder generieren -- Gegnerisches Training -- Ein GAN trainieren -- GANs sind schwer zu trainieren -- Lernziele -- Kapitel 6: Einfache 1010-Muster -- Echte Datenquelle -- Den Diskriminator erstellen -- Den Diskriminator testen -- Den Generator erstellen -- Die Generatorausgabe {\"u}berpr{\"u}fen -- Das GAN trainieren -- Lernziele -- Kapitel 7: Handgeschriebene Ziffern -- Die Datensatzklasse -- Der MNIST-Diskriminator -- Den Diskriminator testen -- MNIST-Generator -- Die Generatorausgabe testen -- Das GAN trainieren -- Mode Collapse -- Das GAN-Training verbessern -- Mit Startwerten experimentieren -- Lernziele -- Kapitel 8: Menschliche Gesichter -- Farbbilder -- Der CelebA-Datensatz -- Hierarchisches Datenformat -- Die Daten abrufen -- Die Daten inspizieren -- Die Datensatzklasse -- Der Diskriminator -- Den Diskriminator testen -- GPU-Beschleunigung -- Der Generator -- Die Generatorausgabe {\"u}berpr{\"u}fen -- Das GAN trainieren -- Lernziele.},
 author = {Rashid, Tariq},
 year = {2020},
 title = {GANs mit PyTorch selbst programmieren: Ein verst{\"a}ndlicher Einstieg in Generative Adversarial Networks},
 url = {https://ebookcentral.proquest.com/lib/kxp/detail.action?docID=6349844},
 keywords = {Electronic books},
 address = {Heidelberg},
 edition = {1. Auflage},
 publisher = {o'Reilly},
 isbn = {9783960103936}
}


@book{Taulli.2019,
 abstract = {Intro -- Contents -- About the Author -- Foreword -- Introduction -- Chapter 1: AI Foundations -- Alan Turing and the Turing Test -- The Brain Is a$\ldots$Machine? -- Cybernetics -- The Origin Story -- Golden Age of AI -- AI Winter -- The Rise and Fall of Expert Systems -- Neural Networks and Deep Learning -- Technological Drivers of Modern AI -- Structure of AI -- Conclusion -- Key Takeaways -- Chapter 2: Data -- Data Basics -- Types of Data -- Big Data -- Volume -- Variety -- Velocity -- Databases and Other Tools -- Data Process -- Step {\#}1-Business Understanding -- Step {\#}2-Data Understanding -- Step {\#}3-Data Preparation -- Ethics and Governance -- How Much Data Do You Need for AI? -- More Data Terms and Concepts -- Conclusion -- Key Takeaways -- Chapter 3: Machine Learning -- What Is Machine Learning? -- Standard Deviation -- The Normal Distribution -- Bayes' Theorem -- Correlation -- Feature Extraction -- What Can You Do with Machine Learning? -- The Machine Learning Process -- Step {\#}1-Data Order -- Step {\#}2-Choose a Model -- Step {\#}3-Train the Model -- Step {\#}4-Evaluate the Model -- Step {\#}5-Fine-Tune the Model -- Applying Algorithms -- Supervised Learning -- Unsupervised Learning -- Reinforcement Learning -- Semi-supervised Learning -- Common Types of Machine Learning Algorithms -- Na{\"i}ve Bayes Classifier (Supervised Learning/Classification) -- K-Nearest Neighbor (Supervised Learning/Classification) -- Linear Regression (Supervised Learning/Regression) -- Decision Tree (Supervised Learning/Regression) -- Ensemble Modelling (Supervised Learning/Regression) -- K-Means Clustering (Unsupervised/Clustering) -- Conclusion -- Key Takeaways -- Chapter 4: Deep Learning -- Difference Between Deep Learning and Machine Learning -- So What Is Deep Learning Then? -- The Brain and Deep Learning -- Artificial Neural Networks (ANNs) -- Backpropagation.},
 author = {Taulli, Tom},
 year = {2019},
 title = {Artificial intelligence basics: A non-technical introduction},
 url = {https://ebookcentral.proquest.com/lib/kxp/detail.action?docID=5845528},
 keywords = {Artificial Intelligence;Electronic books},
 address = {New York},
 publisher = {Apress},
 isbn = {9781484250280}
}


